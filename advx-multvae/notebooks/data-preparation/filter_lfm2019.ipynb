{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f93ab9",
   "metadata": {},
   "source": [
    "# Notebook to filter the LFM2b dataset\n",
    "\n",
    "Prerequisites:\n",
    "Download the files \n",
    "- [listening-events.tsv.bz2](http://www.cp.jku.at/datasets/lfm-2b/chiir/listening-events.tsv.bz2)\n",
    "- [users.tsv.bz2](http://www.cp.jku.at/datasets/lfm-2b/chiir/users.tsv.bz2)  \n",
    "\n",
    "from http://www.cp.jku.at/datasets/LFM-2b/ to your preferred directory and extract them.\n",
    "\n",
    "In case you do not want to reduce the time span of the listening events, unlike us,\n",
    "download [listening-counts.tsv.bz2](http://www.cp.jku.at/datasets/lfm-2b/chiir/listening-counts.tsv.bz2) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b49e8",
   "metadata": {},
   "source": [
    "As the dataset is quite big (~90GB extracted), we filter all listening events that are not relevant for our use-case and will proceed with the preprocessing in another notebook.\n",
    "\n",
    "Our main objective is to reduce the memory consumption when loading the dataset, such that anyone can create the dataset for themselves. We are aware that this leads to longer processing times, however, as this has to be done only once, this should be an acceptible tradeoff. \n",
    "\n",
    "To do so, please execute the following command to split the dataset into multiple smaller files:  \n",
    "```/usr/bin/split listening-events.tsv -l 20000000```  \n",
    "and move all files into a separate folder.\n",
    "\n",
    "The current configuration requieres about 3 GB of free memory and takes ~2h to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedd13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/media/data1/Tmp/lfm\"\n",
    "# data_dir = r\"F:\\Temp\\lfm2b data\"\n",
    "splits_dir = \"/media/data1/Tmp/lfm/splits\"\n",
    "filtered_splits_dir = \"/media/data1/Tmp/lfm/splits_filtered_2019\"\n",
    "# filtered_splits_dir = r\"F:\\Temp\\lfm2b data\\splits_filtered_2019\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0181f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime,  timedelta\n",
    "from scipy import sparse as sp\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136068d",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb864bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time range of which to take interactions\n",
    "min_time = datetime.fromisoformat('2019-01-01')\n",
    "max_time = datetime.fromisoformat('2019-12-31')\n",
    "\n",
    "def date_parser(time_str):\n",
    "    return datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def time_ok(time_str):\n",
    "    return min_time <= datetime.strptime(time_str, \"%Y-%m-%d %H:%M:%S\") <= max_time\n",
    "\n",
    "# filter users & tracks with too less interaction\n",
    "min_interactions_user = 10\n",
    "min_interactions_item = 10\n",
    "min_interaction_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9e8bf",
   "metadata": {},
   "source": [
    "### Preprocess users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591d2aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>RegistrationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>31</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-12-28 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>43</td>\n",
       "      <td>m</td>\n",
       "      <td>2003-04-15 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UK</td>\n",
       "      <td>35</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-10-29 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>BR</td>\n",
       "      <td>31</td>\n",
       "      <td>m</td>\n",
       "      <td>2003-07-20 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>m</td>\n",
       "      <td>2003-07-21 02:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Country  Age Gender     RegistrationDate\n",
       "0       0      UK   31      m  2002-12-28 01:00:00\n",
       "1       1      US   43      m  2003-04-15 02:00:00\n",
       "2       2      UK   35      m  2002-10-29 01:00:00\n",
       "3       3      BR   31      m  2003-07-20 02:00:00\n",
       "4       4     NaN   51      m  2003-07-21 02:00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users = pd.read_csv(os.path.join(data_dir, \"users.tsv\"), \n",
    "                        sep=\"\\t\", engine=\"python\", encoding='latin-1',\n",
    "                        header=0)\n",
    "df_users.columns = [\"UserID\", \"Country\", \"Age\", \"Gender\", \"RegistrationDate\"]\n",
    "\n",
    "# keep users for which at least one attribute is available\n",
    "user_mask = ~df_users[\"Country\"].isna()\n",
    "user_mask |= df_users[\"Gender\"].isin([\"f\", \"m\"])\n",
    "user_mask |= df_users[\"Age\"] > 0\n",
    "\n",
    "df_users = df_users[user_mask]\n",
    "user_ids = df_users[\"UserID\"]\n",
    "\n",
    "# store old user indices and adjust user ids\n",
    "df_users.reset_index(drop=True, inplace=True)\n",
    "user_mapping = {a: b for a, b in zip(user_ids, df_users.index)}\n",
    "df_users = df_users.assign(UserID = df_users.index)\n",
    "\n",
    "df_users.to_csv(os.path.join(data_dir, \"users-demo.tsv\"), sep=\"\\t\", index=False)\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb58dda",
   "metadata": {},
   "source": [
    "### Filter interactions\n",
    "We first select only the users for which some demographic information is available. Moreover, we drop all interactions that did happen in our previously defined time frame. The results will again be stored in separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb3da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|██████████████████████████| 101/101 [23:20<00:00, 13.87s/it]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(filtered_splits_dir, exist_ok=True)\n",
    "\n",
    "first = True\n",
    "fcount = 0\n",
    "header = [\"user_id\", \"track_id\", \"album_id\", \"timestamp\"]\n",
    "\n",
    "item_indices = set()\n",
    "\n",
    "files = list(sorted(glob.glob(os.path.join(splits_dir, \"*\"))))\n",
    "for f in tqdm(files, desc=\"Parsing files\"):\n",
    "    if first:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", engine=\"c\")\n",
    "        first = False\n",
    "    else:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", names=header, engine=\"c\")\n",
    "    \n",
    "    # Filter by user id and timestamp\n",
    "    uid_mask = df[\"user_id\"].isin(user_ids)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "    time_mask = (min_time <= df[\"timestamp\"]) & (df[\"timestamp\"] <= max_time)\n",
    "        \n",
    "    mask = uid_mask & time_mask\n",
    "    if mask.sum() > 0:\n",
    "        df = df.loc[mask, [\"user_id\", \"track_id\"]]\n",
    "        df[\"user_id\"] = df[\"user_id\"].map(user_mapping)\n",
    "        \n",
    "        item_indices = item_indices.union(df[\"track_id\"].unique())\n",
    "        \n",
    "        path = os.path.join(filtered_splits_dir, f\"filtered_part_{fcount}.tsv\")\n",
    "        df.to_csv(path, sep=\"\\t\", index=False)\n",
    "        fcount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1dbba",
   "metadata": {},
   "source": [
    "### Generate interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f135140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|████████████████████████████| 47/47 [00:31<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# gather item indices (in case the cell above is not executed beforehand)\n",
    "item_indices = set()\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(filtered_splits_dir, \"*.tsv\")))\n",
    "for i, f in enumerate(tqdm(files, desc=\"Parsing files\")):\n",
    "    df = pd.read_csv(f, sep=\"\\t\")\n",
    "    item_indices = item_indices.union(df[\"track_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8368b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(df_users)\n",
    "n_items = len(item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0689f055",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_map = {idx: i for i, idx in enumerate(sorted(item_indices))} \n",
    "item_map_rev = {v: k for k, v in item_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a36889c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|████████████████████████████| 47/47 [04:26<00:00,  5.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# generate interaction matrix for the individual files\n",
    "files = sorted(glob.glob(os.path.join(filtered_splits_dir, \"*.tsv\")))\n",
    "for i, f in enumerate(tqdm(files, desc=\"Parsing files\")):\n",
    "    df = pd.read_csv(f, sep=\"\\t\")\n",
    "    \n",
    "    d = defaultdict(lambda: 0)\n",
    "    for uid, iid in zip(df[\"user_id\"], df[\"track_id\"]):\n",
    "        d[(uid, iid)] += 1\n",
    "        \n",
    "    uids, iids = zip(*d.keys())\n",
    "    iids = [item_map[i] for i in iids]\n",
    "    data = list(d.values())\n",
    "    \n",
    "    interaction_matrix = sp.csr_matrix((data, (uids, iids)), \n",
    "                                       (n_users, n_items))\n",
    "\n",
    "    sp.save_npz(os.path.join(filtered_splits_dir, f\"interaction_matrix_part_{i}.npz\"), interaction_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9547a41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing item chunks: 100%|███████████████████| 18/18 [00:28<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# process the items in chunks when loading the previously stored matrix parts\n",
    "items_per_chunk = 500_000\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(filtered_splits_dir, \"*.npz\")))\n",
    "\n",
    "im_full = []\n",
    "valid_item_indices = []\n",
    "chunk_steps = list(range(0, n_items, items_per_chunk))\n",
    "for i in tqdm(chunk_steps, desc=\"Processing item chunks\"):\n",
    "    interaction_matrix = None\n",
    "    for f in files:\n",
    "        im_part = sp.load_npz(f)[:, i : i+items_per_chunk]\n",
    "        if interaction_matrix is None:\n",
    "            interaction_matrix = sp.csr_matrix(im_part.shape, dtype=int)\n",
    "        interaction_matrix += im_part\n",
    "\n",
    "    # Ignore \"misclick\" interactions\n",
    "    interaction_matrix.data -= min_interaction_count\n",
    "    interaction_matrix.eliminate_zeros()\n",
    "\n",
    "    # and binarize result\n",
    "    interaction_matrix.data[:] = 1\n",
    "        \n",
    "    # determine items with enough interactions. this should already reduce the number of items considerably\n",
    "    # allowing us to store them in memory\n",
    "    valid_items = np.argwhere(np.array(interaction_matrix.sum(axis=0)).flatten() >= min_interactions_item).flatten()\n",
    "    valid_item_indices.append(valid_items + i)\n",
    "    im_full.append(interaction_matrix[:, valid_items])\n",
    "    \n",
    "im_full = sp.hstack(im_full).tocsr()\n",
    "valid_item_indices = np.concatenate(valid_item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d0a1992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50813373it [00:38, 1318153.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 297826 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_item_map = {ind: i for i, ind in enumerate(valid_item_indices)}\n",
    "filtered_item_mapping = {item_map_rev[k]: v for k, v in valid_item_map.items()}\n",
    "\n",
    "original_item_indices = set(filtered_item_mapping.keys())\n",
    "    \n",
    "n_items = 0\n",
    "with open(os.path.join(data_dir, \"tracks.tsv\"), \"r\") as csv_input:\n",
    "    header = next(csv_input)[:-1].split(\"\\t\") # remove whitespace\n",
    "    \n",
    "    with open(os.path.join(data_dir, \"tracks_filtered.tsv\"), \"w\") as csv_output:\n",
    "        writer = csv.writer(csv_output, delimiter=\"\\t\")\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        for row in tqdm(csv_input):\n",
    "            (iid, artist, track) = row[:-1].split(\"\\t\")\n",
    "            iid = int(iid)\n",
    "            if iid in original_item_indices:\n",
    "                writer.writerow((filtered_item_mapping[iid], artist, track))\n",
    "                n_items += 1\n",
    "                \n",
    "print(f\"Kept {n_items} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59dfcc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also store track mappings, as the ids may later be used to retrieve genre information\n",
    "ks, vs = zip(*filtered_item_mapping.items())\n",
    "df_item_mapping = pd.DataFrame.from_dict({\"old\": ks, \"new\": vs})\n",
    "df_item_mapping.to_csv(os.path.join(data_dir, \"tracks_filtered_mapping.tsv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b985fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<13593x297826 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10231472 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# moreover, determine which users are lacking interactions.\n",
    "valid_users = np.argwhere(np.array(im_full.sum(axis=1)).flatten() >= min_interactions_user).flatten()\n",
    "im_full = im_full[valid_users, :]\n",
    "sp.save_npz(os.path.join(data_dir, \"interaction_matrix_filtered.npz\"), im_full)\n",
    "\n",
    "df_users = df_users.loc[valid_users, :]\n",
    "df_users.reset_index(drop=True, inplace=True)\n",
    "df_users = df_users.assign(UserID=df_users.index)\n",
    "df_users.to_csv(os.path.join(data_dir, \"users_demo_filtered.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Final matrix\")\n",
    "display(im_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4e42a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>RegistrationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "      <td>31</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-12-28 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>35</td>\n",
       "      <td>m</td>\n",
       "      <td>2002-10-29 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>BR</td>\n",
       "      <td>31</td>\n",
       "      <td>m</td>\n",
       "      <td>2003-07-20 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AT</td>\n",
       "      <td>28</td>\n",
       "      <td>n</td>\n",
       "      <td>2003-07-23 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>UK</td>\n",
       "      <td>48</td>\n",
       "      <td>m</td>\n",
       "      <td>2003-02-18 21:44:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13588</th>\n",
       "      <td>13588</td>\n",
       "      <td>FI</td>\n",
       "      <td>-1</td>\n",
       "      <td>m</td>\n",
       "      <td>2012-05-27 19:05:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13589</th>\n",
       "      <td>13589</td>\n",
       "      <td>NL</td>\n",
       "      <td>-1</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-05-28 17:50:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13590</th>\n",
       "      <td>13590</td>\n",
       "      <td>PL</td>\n",
       "      <td>16</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-05-28 19:59:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13591</th>\n",
       "      <td>13591</td>\n",
       "      <td>BY</td>\n",
       "      <td>19</td>\n",
       "      <td>f</td>\n",
       "      <td>2012-07-19 22:07:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13592</th>\n",
       "      <td>13592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>n</td>\n",
       "      <td>2012-07-25 23:09:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13593 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserID Country  Age Gender     RegistrationDate\n",
       "0           0      UK   31      m  2002-12-28 01:00:00\n",
       "1           1      UK   35      m  2002-10-29 01:00:00\n",
       "2           2      BR   31      m  2003-07-20 02:00:00\n",
       "3           3      AT   28      n  2003-07-23 02:00:00\n",
       "4           4      UK   48      m  2003-02-18 21:44:13\n",
       "...       ...     ...  ...    ...                  ...\n",
       "13588   13588      FI   -1      m  2012-05-27 19:05:13\n",
       "13589   13589      NL   -1      f  2012-05-28 17:50:26\n",
       "13590   13590      PL   16      f  2012-05-28 19:59:37\n",
       "13591   13591      BY   19      f  2012-07-19 22:07:27\n",
       "13592   13592     NaN   16      n  2012-07-25 23:09:06\n",
       "\n",
       "[13593 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26525061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
